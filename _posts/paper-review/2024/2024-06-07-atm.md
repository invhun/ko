---
title: "[논문 리뷰] ATM: Action Temporality Modeling for Video Question Answering"
author: invhun
date: 2024-06-07 15:00:00 +0900
last_modified_at: 2024-06-07 15:00:00 +0900
categories: [Paper Review, Multimodal Learning]
tags: [Text-Video, Retrieval, Text-Video Retrieval, Multimodal Learning, Representation Learning]
use_math: true
pin: false
lang: ko
---

> ATM: Action Temporality Modeling for Video Question Answering    
> ACM MM 2023   
> **Junwen Chen**, Jie Zhu, Yu Kong   
> Michigan State University   
> [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3612509)]

# 1. Abstract & Introduction

![fig1](https://1drv.ms/i/c/af5642aec05791fb/IQtQeSWCOxh-SqeAN8Y10TLeAQJs3mgd7xCydXBDSLqICh0?width=668&height=650)

### 기존 연구 문제점
- 프레임간 인과적/시간적 추론을 필요로 하는 질문들은 여전히 답변을 잘 생성하지 못 함
- Motion이 아닌, 정적인 표현에 대한 bias가 존재하기 때문

### 제안 방법
- 옵티컬 플로우를 재고하여, 이가 장기적인 시간적 추론을 캡처하는데 효과적임을 보여줌
- 모션 중심 contrastive learning을 통해서, 모션 표현 강화
- 비디오를 일부러 shuffle하고 질문에 답하지 못하게 하여, static한 feature에 대한 bias를 방지하고 신뢰성있는 시간적 추론을 가능하게함 

# 2. Related Works
### Static Bias in Video
- 비디오는 이미지 수준의 정적 콘텐츠(장면, 객체, 사람)을 이해하는 것에서 벗어나, 여러 이벤트의 시간적 추론 능력을 평가하는 것이 중요함
- 많은 비디오(+언어) task(action recognition, retrieval, etc...)에서 기존의 이미지 중심 모델로 단일 프레임만 사용하고도 다중 프레임을 사용하는 모델과 비견될 성능을 보이는데, 이는 비디오 표현이 static 외형 정보에 편향되어 있음을 의미함
- Static 편향을 해결하기 위한 연구가 진행되었고, VideoQA에서 특히 중요하나, 기존 연구들은 편향을 해결하지 못 하였음
- 저자는 action, temporality를 효과적으로 학습하기 위한 방법을 소개함 이전 action recognition 연구에서 사용되었던, "appearance-free stream"을 모션 표현 향상에 다시 사용하였음

# 3. Methodology
![fig2](https://1drv.ms/i/c/af5642aec05791fb/IQtPfe8rQeF0RbfBJGVWViS_AfstwZYRb8N78pz9O2rh7OA?width=1024)
- Two-stage 학습 방식
    - 1단계: action parser로 question에서 action만 뽑아와서 contrastive learning
    - 2단계: cross entropy, confusion 두 가지 로스를 사용하여, 답변 생성을 위한 학습 수행


## 3.1. Preliminaries
![fo1](https://1drv.ms/i/c/af5642aec05791fb/IQTNhD_rOTDgTZRWa2COTiDAASblwZKXulGJI98g6nx9oMM?width=1024)
- 비디오 $h$와 질문 $q$가 주어졌을 때, VideoQA 작업은 $h$와 $q$를 결합하여 후보 답변 집합 $A$중에서 가장 잘 맞는 답변 $a^*$를 예측하는 것을 목표로 함.

![fo2](https://1drv.ms/i/c/af5642aec05791fb/IQTXdNC3aa0zRJ0MRCiek9FoARrhQWaSYi8S4UYaMxxaL3I?width=462&height=54)
- 최근 연구 VGT와 VQA-T와 동일하게, $F_W$를 모달 별 독립적인 트랜스포머를 구성하고, visual-text 유사도를 계산하는 방식으로 설계함.
- $F_v$: video encoder, $F_q$: text encoder, $[:]$는 concat

![fo3](https://1drv.ms/i/c/af5642aec05791fb/IQaapB8MhYKvSpzOEE_4HSSTAUU166rV5wZau7b0-DBFUSY?width=874&height=106)
- dot product로 유사도를 구하고, 최대가 되는 답변후보를 최종 답변으로 결정
- 당시 SOTA인 VGT와 동일하게, $F_q$는 BERT로, $F_v$는 프레임 특징 추출기 $f_r$과 객체 특징 추출기 $f_o$로 구성됨

## 3.2. Rethinking motion representations in VideoQA
![fig3](https://1drv.ms/i/c/af5642aec05791fb/IQtHvfAusKYpR4nctcXc_iusAZ_AlkhthumJ-OzjwQltMEw?width=1024)
![fo4](https://1drv.ms/i/c/af5642aec05791fb/IQTfAyq9UJgMTYGQSBVxcMbFAdMNP1tPbsAG7pWnZiGneyA?width=1024)
- Pretrained 3D Conv를 사용하여, 인접한 프레임 간 temporality를 추출하는 연구가 많음. 하지만 VideoQA task에서는 프레임 전반에 걸쳐 수행되는 시간성이 중요함. 따라서 이 방법은 적절하지않음.
- 또한 이웃한 RGB frame은 동작을 정확하게 모델링하기에는 redundancy함. 그림 3(a)에서 "기차가 멈추고 있다"를 RGB에서는 인식하기 어려운 반면, 옵티컬 플로우에서는 명확하게 드러남.
- $f_r$, $f_o$에 더불어 kinetics-400으로 사전학습된 백본을 사용하여 추출된 플로우 특징 $f_m$을 사용하여, MLP, positional embedding, MSA, mean-pool을 거쳐 최종 비디오 표현 계산함
- 그림 3 (b)와 같이 Action recognition task에서는 RGB기반 백본이 높은 성능을 달성하나, VIdeoQA에서는 저조하고, 반대로 "appearance-free stream"인 옵티컬 플로우 방법이 유용함

## 3.3. Action-centric Contrastive Learning (AcCL)
- 그림 3(b) 참조
- Action은 추론에 중요한 부분이지만, Q-A pair은 캐릭터, 객체, 위치를 포함하여 많은 정보가 있기 때문에, action이 학습 중에 무시될 수 있음
- Action parser를 사용하여, Question에서 Action만 추출하고, Contrastive loss를 사용하여 두 모달리티의 인코더를 fine-tuning함
- 이때 배치 내 다른 샘플의 action들을 negative-pair로 사용하여, Contrastive learning을 수행함

## 3.4. Temporal Sensitivity-aware Confusion Loss (TSC)
![fo6](https://1drv.ms/i/c/af5642aec05791fb/IQTl6OYD_MSCQohkCJ_cYd7EAa7cqp-7HLaS8Oj26V19Y0U?width=1024)
![fo6.1](https://1drv.ms/i/c/af5642aec05791fb/IQcujmq7cxAURIXLj6sixokXAZr719wTaQpHzu5NPL276po?width=1024)
![fo7](https://1drv.ms/i/c/af5642aec05791fb/IQaQxzQtJv2ERI2cYYHjPWmsAQhl-2nendRoWiT7D4jT0Kc?width=1024)
![fo8](https://1drv.ms/i/c/af5642aec05791fb/IQcigg4Il8QxSrf2N_4JKIFmAYtLUHrMHVL9IlPJ-P69S0k?width=1024)

- 옵티컬 프레임이 RGB 피처와 fused 되었기 때문에, 여전히 scene/object로의 Bias가 존재함
- 시간적으로 뒤섞인 프레임을 주었을 때, 모델이 답을 틀리도록 유도함으로써, bias를 감소하고, 올바른 시간적 추론을 가능하게 함
- 즉 "What is the train doing after moving for a while?" 이러한 질문이 왔을 때, 비디오 프레임을 랜덤하게 섞는다면, 정답을 못 맞추는 것이 정상이지만, 맞춘다면, 시간적으로 추론을 한 것이 아니고, 편향에 의한 것이기 때문에, 이를 억제하게 하는 것
- 따라서 수식 6과 7에 따라 답변후보의 분포가 균등하게 되도록 즉, Entropy가 최대가 되도록 학습을 진행함. 단, "before", "after", "when"과 같은 syntax를 포함한 temporal-senstive한 question에만 적용함
- 그리고 전체 Question에 대해 모델이 예측한 답변 확률이 실제 답변과 얼마나 일치하는지를 평가하는 CE 손실을 추가로 적용 (8번 수식)

# 4. Experiments
# 4.2. Implemenation details
- 비디오 피처: 16clip, clip당 4프레임 사용
- 모션 피처: 원본 FPS를 사용하여 DenseFLow를 통해 옵티컬 플로우 맵을 추출 후 Kintetics-400으로 학습된 mmaction2 based ResNet을 사용하여 옵티컬 플로우 피처 추출
비디오당 16개의 clip, clip당 5개의 프레임을 샘플링하여 uniform하게 분포, clip당 총 2048차원의 피처 벡터를 구성
- SpaCy parser를 사용하여, question에서 action 추출
- A6000 1대, 최대 10에폭, 배치사이즈 64
- Next-QA 중 17,681개의 question은 temporal-sensitive하고 16,451은 insensitive함
- TGIF의 "action", transition"은 모두 temporal sensitive함
- 나머지 TGIF와, MSRVTT은 모두 insensitive함

# 4.3. Comparison with State-of-the-Art

![t1](https://1drv.ms/i/c/af5642aec05791fb/IQt7LkXNsl1IQ4BeaoyV0L5KASNd_QXDqm8wn874K2fBa2o?width=1024)
![t2](https://1drv.ms/i/c/af5642aec05791fb/IQaSgmIk3SeDTb8BDy7Vp-vQAXzVjjw-edmDOmevm8RAkjs?width=1024)
- Next-QA 데이터셋 성능: ATM은 외부 데이터 사전 훈련 없이 모든 기존 방법보다 더 나은 성능을 보임.
- 시간적 추론 효과성: 다양한 질문 유형(짧은 세그먼트에서 전체 비디오까지)에 대해 효과적임을 입증
- ATM은 대규모 사전 학습을 사용한 방법을 초월함.
- 모션 모델링: ATP보다 3.97% 더 나은 성능을 보여주며, 시간적 비중이 높은 작업에서 더 효과적임.
- TGIF-QA 성능: 반복 행동 및 객체 전환 시나리오에서 SOTA 달성
- MSRVTT-QA 성능: 사전 학습이 없는 VGT보다 우수하지만, 대규모 사전 학습방법보다는 저조

# 4.4. True Temporality Metric
![t3](https://1drv.ms/i/c/af5642aec05791fb/IQaYhEi90qDxRL9048FhRvygAdKQYHp7D5Wd2IN6oOyoW00?width=1024)
- QA 정확도 차이 측정: 전체 비디오를 제공했을 떄와 중간 프레임 한개를 제공했을 때의 QA 정확도 차이를 $\delta$ 로 측정.
- $\delta$ 의 차이로 부터 외부 대규모 데이터가 정적인 정보를 더 많이 활용하도록 유도함을 확인할 수 있음
- 제안 방식의 경우 큰 차이가 있으며, 각각의 모듈의 성능 향상이 확실하게 존재함

# 4.5. Ablation Studies

![t4](https://1drv.ms/i/c/af5642aec05791fb/IQTJyOW5UynUS7CEYGIYfuZRATVmZsffNIZQxbgxU2q2M-Q?width=1024)
- Impact of Action-centric Contrastive Learning: AcCL이 다른 변형보다 우수한 성능을 보이며, 질문의 action 구문이 시간적 추론에 중요함을 입증.
- Impact of TSC Loss: TSC를 사용할 때 VideoQA 정확도와 True 시간성 추론 메트릭에서 성능이 향상됨.
- Impact of Appearance-free stream: Flow 맵을 포함한 방법이 RGB-only 방법보다 VideoQA에서 더 효과적이며, 클립 수가 16일 때 최적의 정확도 달성.

# Review
- 정적인 이미지에 편향되어있다는 문제점과 그에 따른 해결책이 명확하고, 실험도 명확하게 보여줌
- 또한 예전 방식은 optical flow를 사용하는 이유에 대한 시각화가 잘 구성되어서 직관적으로 와닿음
- 다른 비디오 task에도 일부 차용하여 적용이 가능할 것으로 판단됨