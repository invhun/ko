---
title: "[논문 리뷰] Weighted Point Cloud Embedding for Multi-modal Contrastive Learning Toward Optimal Similarity Metric"
author: invhun
date: 2025-03-03 16:00:00 +0900
categories: [Paper Review, Multimodal Learning]
tags: [Text-Image, Multimodal Learning, Contrastive Learning, Representation Learning]
math: true
pin: false
---

> Weighted Point Cloud Embedding for Multi-modal Contrastive Learning Toward Optimal Similarity Metric   
> ICLR 2025 spotlight   
> **Toshimitsu Uesaka**, aiji Suzuki, Yuhta Takida, Chieh-Hsin Lai, Naoki Murata, Yuki Mitsufuji   
> Sony AI, The University of Tokyo, RIKEN AIP, Sony Group Corporation     
> [[paper](https://openreview.net/pdf?id=uSz2K30RRd)]

# 1 Abstract & Introduction

### 기존 연구 문제점

- CLIP 모델은 각 입력에 대해 하나의 점 임베딩을 생성하여 실제 세계의 복잡한 관계와 유사성 구조를 포착하는 데 한계가 있음.
- 유사성 구조가 코사인 유사성에 의존하여 표현의 풍부함이 제한됨.

### 제안 방법
- 가중치 포인트 클라우드(Weighted Point Cloud Embedding, WPCE)을 개념의 표현으로 제안, 스칼라 가중치와 벡터 점의 쌍 집합을 사용.
- 두 포인트 클라우드의 유사성을 정의하는 커널 함수를 도입, 이것이 코사인 유사성보다 더 풍부한 표현 능력을 제공함을 증명.
- 대칭 InfoNCE 손실을 통해 최적 유사성이 점별 상호 정보(point-wise mutual information)로 표현될 때 손실 최소화됨을 강조.


# 2 Related Work

## 2.1 Multimodal Contrastive Representation Learning in Practice
- InfoNCE loss를 기반으로한 연구는 제로샷, downstream task에서 효과적이나, 유사성 구조의 다양성이 부족
- 이러한 유사성 구조를 개선하기 위해, 현대 홉필드(HopField)네트워크를 적용하거나(-> InfoLOOB), Lorentzian 거리를 사용하는 방식이 제안됨.
- [[MERU](https://arxiv.org/pdf/2304.09172)]에서는 계층 구조를 포착하기 위해 하이퍼볼릭 공간에서 Lorentzian 거리를 유사성으로 사용하는 방법을 제안
- 저자는 이러한 연구를 따라, 비선형 커널과 가중치 포인트 클라우드 기반의 유사성을 통해 확장

![meru_fig1](https://1drv.ms/i/c/af5642aec05791fb/IQQq4gMahzbpS7muJL-e8uf3AaA6AKJVEDPtRo14ltpcM8g?width=480&height=490)
![meru_fig2](https://1drv.ms/i/c/af5642aec05791fb/IQS7yJXvtjbMRKHli_eS0lpZAbHkBhZIGedw3RM-7NmSkvU?width=482&height=414)

## 2.2 Theoretical Understanding of Contrastive Loss
- 저자는 infoNCE loss가 소개된 후, 이어온 대조학습의 관련 연구를 소개하였는데, 대부분 읽어보지 않은 논문들이였음. 나는 아직 대조학습에 대해 아는게 거의 없구나...라고 생각하게 됨
- 저자는 세가지 기존 연구의 문제점을 제시하며, 연구의 차별점을 주장함
    1. Downstream Loss의 Upperbound: 
    일부 연구의 경우 downstream loss의 상한만 제시하는데, 상한과 최적값 간에 차이가 있을 경우, 로스를 줄이는 것이 성능 향상을 보장하지 않음
    -> 실제 성능 향상에 대한 불확실성이 존재함
    -> 저자는 상한뿐 아니라 상한과 최적 분류기간의 차이도 고려함
    2. 이론적 분석 대상 변경
    일부 연구는 일반적인 대조학습과는 다른 특징에 대한 guarantee를 보장을 제공함으로써, CLIP의 실제 작동 방식과 동떨어진 결과를 도출할 수 있음
    -> 이론적 분석이 실제 적용에 얼마나 유효한지에 대한 의문점 제기
    -> 저자는 CLIP의 실제 설정과 유사한 접근 방식을 사용하여, infonCE & classifier를 분석
    3. 통계 분석
    기존 연구는 다양한 통계(e.g. 분산)을 제공하는데, 이러한 통계는 정렬이 완벽히 이루어졌을 때 유용하지만, 이는 현실적이지 않음
    -> 저자의 이론젹 결과에 대한 가정은 상대적으로 mild함
    -> 멀티모달 학습에서 발생할 수 있는 다양한 현상들을 더 잘 반영할 수 있게 함.

# 3 Problem Setup
## 3.1 Contrastive Representation Learning and Symmetric InfoNCE
![fo1](https://1drv.ms/i/c/af5642aec05791fb/IQTUXrv5bakbRb9-QYKHh8xdAWBwZYYk4G4pTDk92qlB0CE?width=501&height=108)
![fo2](https://1drv.ms/i/c/af5642aec05791fb/IQSVQjfklFmFSps7_dtoZyCpAYg35H-ELP-EBeftHQKZtb4?width=603&height=42)

- InfoNCE 수식에 설명은 생략함
- N-> $$\infty$$ 때의 대칭 infoNCE의 모집단 기대값 형태를 제시함

## 3.2 Downstream Classification Task
- Supervised 방식의 softmax cross entropy 로스에 대한 수식 정리 내용으로 생략함

# 4 Theoretical Gurantee via Pointwise Mutual Information
## 4.1 Pointwise Mutual Information as Optimal Similarity
- 이미지(X)와 텍스트(Y)간의 Pointwise Mutual Information (PMI)는 특정 데이터쌍이 서로 얼마나 관련이 있는지를 나타내는 것으로, 수식으로 표현하면 다음과 같음
$$I(X, Y) = E_{p(x,y)} \left[ \ln \frac{p(x,y)}{p(x)p(y)} \right]$$
- PMI는 infoNCE 손실의 상한을 나타내며, 유사성 g가 $$g(x, y) = \ln \frac{p(x,y)}{p(x)p(y)} + \text{const}$$, 이를 만족한다면, $$I(X, Y) = -L_{NCE}(g)$$ 이 두값은 같아짐.
- 이 최적 유사성을 $$g^*(x,y)$$ 로 나타냄

## 4.2 Pointwise Mutual Information Estimator Leads to a Good Linear Classifier

- 특정 조건 하에, 최적 유사성을 달성하는 인코더를 성공적으로 얻으면, 학습된 표현에 대한 classifier가 optimized classifier에 가까워짐을 보여줌.
![fo3](https://1drv.ms/i/c/af5642aec05791fb/IQQIMHsxUjSfSbluoBCKQL7lAT14oGIx7DO8sANcZsTmbW0?width=636&height=37)
- classifier의 로스, cross entropy $$H(·,·)$$, 
- 저자는 Excess Risk(학습된 모델의 성능이 최적의 성능과 비교했을 떄 얼마나 나쁜지)를 측정하였음

![theorem4.2](https://1drv.ms/i/c/af5642aec05791fb/IQQIVZEpL7WOQ67_Q-prbNRwAYgah-P0CUIV6xzYw2fo_Rw?width=776&height=141)
- 전체 라벨에 대해 서로 겹치지 않는 부분집합을 선택하고 인코더가 최적 유사성이 성립한다고 가정하였을 때, 최적의 classifier와의 excess risk가 최소가 됨
- 첫번째 KL의 경우, 레이블이 주어졌을 때, 해당 레이블의 조건부 확률이 텍스트 데이터 x에 대한 조건부 확률과 잘 맞는 경우에 최적의 성능을 발휘
- 두번째 KL의 경우, y가 Y_c에 주어진 x와 독립적일 때, 0이 됨.

## 4.3 Excessive Risk Analysis via the Gap from Pointwise Mutual Information

- PMI와 같은 최적의 유사도가 classifier의 excess risk를 낮추는데 기여함을 확인하였지만, 실제 유사도는 이러한 최적 유사도와 다를 수 있음.
![fo4](https://1drv.ms/i/c/af5642aec05791fb/IQRnd6shHMKeQIkLnWxRqM_0AcUYXL4mKcUGnZMr9NLRXa8?width=696&height=49)
- 리스크를 1. 실제 유사도와 최적 유사도 간의 차이로 인한 리스크, 2. 앞선 정리로 인해 bound가 설정된 항. 2가지로 구분
![theorem4.4](https://1drv.ms/i/c/af5642aec05791fb/IQSEDJdh28W3Qb8-qG9yXyqWAcy8FWzeJiu3gF2HaWYfkmI?width=786&height=219)
- 정리 4.4는 최적 유사도와 실제 유사도 간의 차이가 classifier의 성능에 영향을 미칠 수 있음을 보여주며, 차이가 클 수록 분류기의 excess risk가 커짐을 보여줌

# 5 Augmented Similarity by Weighted Point Clouds

- PMI를 근사하는 유사도의 한계를 살펴보고, 이를 극복하기 위한 새로운 유사도 클래스를 제안

## 5.1 Limitation of the Inner-product Similarity in Finite Dimensional Spaces

- 문제설정
(d)-차원 특징 공간을 고려. $$N (> d + 1)$$ 쌍의 샘플 $$(x_1, y_1)_, \ldots, (x_N, y_N) \in X \times Y $$가 있다고 가정.
- 특징을 다음과 같이 정의: $$Z_X := [f_X(x_1), \ldots, f_X(x_N)], \quad Z_Y := [f_Y(y_1), \ldots, f_Y(y_N)]$$.

- 유사도 매트릭스:
    대칭 InfoNCE를 사용한 사전 훈련 동안, 유사도 매트릭스 $$Z_X^{\top} Z_Y$$는 최적 유사도 매트릭스 $$G \in \mathbb{R}^{N \times N}$$에 맞추어 조정. $$G$$의 요소는 다음과 같이 정의됨: $$G_{ij} = \ln \frac{p(x_i, y_j)}{p(x_i) p(y_j)}$$.

- 근사 오류 분석:
    유사도 간의 차이 $$\Delta$$는 다음과 같이 정의: $$\Delta \geq \sup_{x \in \text{supp } p(x), y \in \text{supp } p(y)} |g(x, y) - g^*(x, y)| \geq \sup_{i,j} \left[(Z_X^{\top} Z_Y){ij} - \Gamma - G{ij}\right]$$.
    여기서 $$\Gamma$$는 상수.

- 랭크 제한:
다음과 같은 조건이 성립: $$\text{rank}(Z_X^{\top} Z_Y + \Gamma J) \leq d + 1$$, 여기서 $$J$$는 모든 요소가 1인 매트릭스. 이는 유사도 매트릭스의 랭크가 $$(d + 1)$$ 이하로 제한됨을 보여줌.

- 결론:
만약 $$G$$의 랭크가 $$N > d + 1$$이라면, $$G$$의 근사에서 오류가 발생할 수 있음. 즉, 점별 상호 정보를 완전히 포착하기 위해서는 피처 차원 $$d$$가 데이터 공간의 고유 인스턴스 수보다 커야 하며, 이는 실제 상황에서는 비현실적임.

## 5.2. Augmented Similarity by a Nonlinear Kernel and Weighted Point Clouds

![fig1](https://1drv.ms/i/c/af5642aec05791fb/IQSY0xE3_uZLSJS5TV9IP2prAZBluQBTyRurHj8oi4R9piM?width=767&height=357)

- infoNCE에서 유사도를 두 개의 가중치가 있는 point cloud간의 유사도로 대체함
- 인코더는 단일 벡터 대신 M 쌍의 가중치와 벡터로 구성된 가중치가 있는 포인트 클라우드를 생성하도록 수정: $$\lbrace(w_i, v_i)\rbrace_{i \in [M]}.$$
- 두 개의 포인트클라우드 $$\lbrace(w_i^{(X)}, v_i^{(X)})\rbrace_{i \in [M^{(X)}]}$$, $$\lbrace(w_i^{(Y)}, v_i^{(Y)})\rbrace_{i \in [M^{(Y)}]}$$.
- 커널함수 $k$에 대해, 두 포인트 클라우드 간의 유사도는 다음과 같음
![fo6](https://1drv.ms/i/c/af5642aec05791fb/IQSFwzu0tJLZRaiImr4lKD7OAbR4F3QatbHahz7ckDAJnLs?width=748&height=61)
- 즉 각 점의 가중치의 곱에 두 벡터의 비선형 커널함수를 적용하고, 모든 쌍에 대해 합산하여 계산함

## 5.3 Implementation
- ViT: CLS토큰 뿐만 아니라, 모든 패치의 아웃풋 벡터를 다 사용함 -> point cloud
- Text 인코더: 마찬가지로 EOS 뿐만 아니라, 모든 벡터를 사용
- 커널함수: 선형 커널과 비선형 커널의 선형 결합을 사용
- 커널 함수만으로 모델을 학습하였을 때, InfoNCE 손실이 수렴하지 않았고, 이를 멀리 떨어진점에 대한 gradient 소실이라고 가정
-> random Fourier feature(RFF)을 사용하여, 비선형 커널을 근사함 $$z(u)^\top z(v) \approx \tilde{k}(u, v)$$, 이를 통해 Random Fourier feature를 구성
- 점들의 가중합과, RFF의 가중합을 같이 사용

# 6 Experiments
## 6.1 Pretraining
- CC3M, CC12M 사용
- ViT-B/16을 사용
- 비선형 커널: 가우시안 커널과 IMQ 커널 사용
- RFF의 차원을 1024으로 설정

## 6.2 Zero-shot Transfer
![t1](https://1drv.ms/i/c/af5642aec05791fb/IQQcr_YXFu2sQayGGOYMepT6Aa-DRt17QD127oldvLXKqN8?width=783&height=263)
- RFF 차원 512로 설정
- (bef): 이미지 인코더의 마지막 프로젝션 레이어 직전의 잠재 벡터를 사용
- 두 가지 경우 빼고 CLIP보다 우수한 성능을 보이나, 커널 함수에 따라 차이가 큼

## 6.3 Linear Classification
![t2](https://1drv.ms/i/c/af5642aec05791fb/IQTzpqGy1m5pS4Fuj19uMCFbAYXqHm760_qRcnw0R4KHUyw?width=775&height=374)
- 설정은 6.2와 동일
- 대체적으로 CLIP보다 우세한 성능, 커널 함수에 따라 차이가 크며 6.2와 비슷한 경향을 보임

## 6.4 
![t3](https://1drv.ms/i/c/af5642aec05791fb/IQSrTTn_v7nfTqW9Rs146TnTAcpBw_NKzJ2753G6eHZ0go8?width=393&height=176)

- WPCE with positive weights: 모든 가중치를 양수로 설정, 가중치 인코더의 마지막 활성화 함수로 $$ 100 \cdot \text{Sigmoid}(\cdot / 100)$$를 사용했습니다.
- WPCE Linear: 가중치가 있는 포인트 클라우드를 출력하지만 유사도 계산에 선형 커널만 사용합. 즉, $$(\alpha_1, \alpha_2)$$가 $$(1, 0)$$으로 설정됨, (반대의 경우는 NaN이 떠서 실패)
- 표 3은 표1,2와 동일한 데이터셋에 대한 평균 성능을 나타냄
- 음수 가중치가 좋은 선응을 위해 필수적임
- 비선형 커널의 사용이 classification에서 효과적임


# 리뷰
- 결과적인 제안 아이디어 자체는 간단한 방식이며, 모든 패치 토큰을 사용하는 거는 독창적인 방식은 아님
- 하지만, 기존 연구의 문제점을 수식으로 잘 증명하였고, 제안하는 방식의 필요성과 효과가 있음을 이론적&실험적으로 잘 보여주었음 

