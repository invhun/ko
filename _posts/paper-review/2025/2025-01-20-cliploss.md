---
title: "[논문 리뷰] CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning"
author: invhun
date: 2025-01-20 19:00:00 +0900
last_modified_at: 2025-01-20 19:00:00 +0900
categories: [Paper Review, Multimodal Learning]
tags: [Data Selection, Multimodal Learning, Contrastive Learning, Text-Image, Pretrain]
use_math: true
pin: false
---


> CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning   
> NeurIPS 2024 spotlight    
> **YipingWang**, **Yifang Chen**, Wendan Yan, Alex Fang, Wenjing Zhou, Kevin Jamieson, Simon Shaolei Du    
> University of Washington, University of Michigan    
> [[paper](https://arxiv.org/abs/2405.19547)] [[github](https://github.com/ypwang61/negCLIPLoss_NormSim)]

# Abstract
- Data Selection은 대규모 비전-언어 모델(e.g., CLIP) 사전학습에서의 핵심 문제이고, 세가지 주요 접근법이 존재함
    1. 외부 non-CLIP 모델을 활용하여 선택을 돕는 방법
    2. 고품질 데이터로 새로운 CLIP 모델을 학습하는 방법
    3. CLIP임베딩에 보편적으로 적용가능한 평가지표(e.g., CLIPScore) 설계

- 저자는 세번째 접근법이 충분히 탐구되지 않아서, 이에 집중한 방식 제안
    1. negCLIPLoss: 단일 샘플의 두 모달리티 간 정렬만 고려하는 CLIPScore 대신, 대조쌍과의 정렬을 추가적인 정규화 항으로 추가하여 품질 측정을 개선
    2. NormSim: 다운스트림 태스크에 알려진 경우, 사전학습 데이터와 목표 데이터 간의 유사성을 측정하기 위한 정규화 기반 매트릭

- 실험결과
    - DataComp 벤치마크, Image-Net-1k에서 우수한 성능을 보임
    - negCLIPLoss, NormSim은 기존 기술과 호환이 가능함

# 1. Introduction
- 웹 기반 데이터 쌍의 품질은 중요하며, 기존 연구에 따르면 데이터셋의 선택은 모델과 학습 방법에 상관없이 모델 성능에 상당한 영향을 미침
- 전통적 방법:
    - OpenAI의 사전 훈련된 CLIP 모델에 의존
    - CLIPScore: 임베딩 간 코사인 유사도 측정으로 저품질 데이터 제거

- 최근 데이터 필터링 방법
    - 외부 non-CLIP 모델 활용
        - HYPE: 쌍곡선 모델의 임베딩을 활용하여, 각 데이터 포인트가 다른 데이터 포인트와 얼마나 의미적으로 겹치는지 측정
        - T-MARS: 상용 OCR 모델 'FAST'를 사용하여 이미지 내 텍스트와만 상관관계가 존재하는 이미지를 제거
        - Devil: non-English 텍스트를 제거, BLIP-2를 사용하여 유용한 숫자 정보가 있는 이미지를 유지
    - 고품질 데이터로 새로운 CLIP 학습
        - DFN: HQITP-350M과 같은 고품질 데이터셋을 사용하여, CLIP 스타일 교사 모델을 학습 -> 다운스트림 작업의 성능은 떨어지나, 저품질 데이터 필터링은 우수

- 제안 방법:
    - NegCLIPLoss: 정보가 적은 샘플이 편향을 가질 수 있음. 예를 들어 'image'와 같은 단어가 포함되면, 어떤 시각적인 부분과도 높은 유사성을 가질 수 있음(높은 CLIPScore). 따라서 이러한 편향을 줄이기 위해 대조쌍과의 유사성으로 정규화를 수행. 이러한 방법은 다양한 임베딩 모델에 보편적으로 적용이 가능함
    - NormSim: 타겟 task과 동일한 분포에서 추출된 샘플에 접근할 수 있다면, 이 지식을 활용하여 데이터 필터링 과정에 정보를 제공할 수 있음. 학습 샘플과 타켓 task 데이터셋 간 시각적 유사성을 측정하기 위한 p-norm을 사용한 평가지표를 제안. 이 방법은 다양성을 명시적으로 고려하지 않고, 타켓 샘플과 가까운 에제를 선택하는 방식임

- 제안 방식은 성능 개선이 이뤄지며, 기존 방식과의 결합이 가능하며, 다양한 임베딩 모델에 보편적으로 적용이 가능함. 또한 reprocessing, 새로운 임베딩 재학습에서 time complexity 절약 효과가 있음

## 기존 연구 문제점
- 데이터 선택의 중요성: 대규모 시각-언어 모델의 사전 훈련에서 데이터 품질이 모델 성능에 미치는 영향이 큼
- 전통적인 접근법의 한계: 
    - CLIPScore와 같은 전통적인 방법만 사용하여 CLIP 임베딩을 비최적의 방식으로 활용.
    - 저품질 데이터의 필터링이 제한적이며, 임베딩의 잠재력이 충분히 탐구되지 않음.
- 외부 모델 의존성: 최신 데이터 필터링 방법들이 외부 자원에 의존하여 데이터 선택을 수행함.

## 제안 방법
- negCLIPLoss: CLIPScore의 대안으로, 데이터 품질을 더 정확하게 측정하기 위한 방법.
- NormSim: downstream task에 대한 정보가 있을 때 유사성을 측정하는 새로운 분포 평가지표.
- 제안 방법은 OpenAI CLIP뿐만 아니라 다른 CLIP 스타일 모델에도 적용 가능.
- negCLIPLoss, NormSim은 상호보완적으로 적용이 가능함/

# 2. Problem Setup
### Data Filtering on Multiomdal Dataset.
- 주어진 학습 데이터셋 $D_{train}=\lbrace x^v,x^l \rbrace ,x^{vl}\in{x^v,x^l}$에서  제로샷 성능을 극대화하는 서브셋 $S\subset D_{train}$를 식별하는 것

### CLIP socre and embedding.
- Vanila CLIP: $\bar{f}_{vl}$
- Normalized unit vector: $\bar{f}_{vl}(x^{vl})$
- Dataset samples: $X^{vl}:=\lbrace x^{vl}\_{1},\ldots,x^{vl}\_{m}\rbrace$
- CLIPScore: $\langle \bar{f}\_{v}(x^v),\bar{f}\_{l}(x^l)\rangle \in [-1,1]$

### Dataset and model
- 이전 데이터 선택 연구들에서 널리 사용되는 testbed인 DataComp의 파이프라인을 따라 학습 및 평가를 수행

# 3. Data Filtering Strategy
## 3.1. negCLIPLoss: A better Metric than CLIPScore

- CLIPScore를 대체하는 통계적으로 해석 가능한 품질 평가지표 negCLIPLoss(기존 CLIP loss에 영감을 받음)를 제안
- 추가적인 외부 데이터 수집은 필요 없고, 작은 계산 비용이 추가됨
- CLIP Loss 수식:
    ![fo1.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgRxfVXKsjW5_gtf8?embed=1&width=863&height=74)
    $B^*$: i번쨰 샘플이 속하는 랜덤 배치, $\tau$: 학습 가능한 온도변수
- CLIP Loss와 CLIPScore의 차이점인 정규화 Term:
    ![fo1-1.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgR1NBvigBd6C_Hf1?embed=1&width=907&height=111)
- negCLIPLoss 수식:
    ![fo2.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgR5b6G401p6Co5b9?embed=1&width=756&height=69)
    학생 모델의 학습 데이터에서 $K$개의 배치를 선택. 이 연구에선 $K=10$
    배치사이즈와 온도변수는 사전학습된 teacher로부터 얻음
    즉 CLIPScore와 차이점은 배치 내 샘플들과의 정규화 항

### Motivation behind negCLIPLoss.
- 기존 연구들(NLP에서 LESS, CV에서 CoDis 등)에서 Loss기반 데이터 선택을 사용하고 있음
- 멀티모달 대조학습에서도 이러한 교사 손실 기반 선택이 효과가 있음을 보임
    ![fig2.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgRl-eEqHCw15GM5Z?embed=1&width=400&height=566)
    - [Fig.2]에서 negCLIPLoss가 CLIPScore보다 일관되게 더 나은 성능을 보이는 것을 확인할 수 있음
- 정규화 항의 역할:
    ![fig1.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgRuDMjnPS-Pofb4C?embed=1&width=925&height=631)
    - 높은 정규화 항은 이미지 또는 텍스트 임베딩이 여러 대조 쌍과 쉽게 일치할 수 있음을 나타냄
    - 'Image', 'Photo'가 포함된 텍스트는 다양한 이미지와 일치할 수 있음
    - [Fig.1] 'verloopring'의 경우 'white', 'empty', 'circle'과 같은 여러 단어와 일치할 수 있는 경우 높은 CLIPScore와 반대로 상대적으로 낮은 negCLIPLoss를 가짐
    - 반대로 [Fig.1] 왼쪽하단은 텍스트와 이미지 모두에서 구체적인 요소를 특징으로 하여 대조쌍과 일치할 가능성이 적어서 높은 negCLIPLoss 점수를 가짐 
    - 이로 인해 더 정확한 데이터 선택을 가능하게 함

## 3.2. NromSim: A New Training-Target Similarity Metric
- 타켓 downstream task에 접근할 수 있을 때 p-norm 유사성을 사용하여, 학습 샘플과 downstream task 간의 관계를 측정하는 새로운 평가지표
- NormSim 수식:
    ![fo3.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgR91at3yEF9Vjy3p?embed=1&width=827&height=83)
    - $X_{taget}=\lbrace x_{target,(1)},\ldots,x_{target,(m)}\rbrace $, where $x_{taget,(i)} \in \mathbb{R}^{d}, i.i.d.-sampled$ from 타겟 downstream task 분포
    - 저자는 p-norm 유사성 기준 상위 N개의 샘플을 선택하여 서브셋을 결정
    - $p=2$일 경우 (L2norm): 
        ![fo4.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgSCeWta8AhuKjxUx?embed=1&width=890&height=89)
        - 타겟 집합 분산의 주성분과 정렬된 서브셋을 선택하는 것과 동일(자세한 수식적 분석은 appendix참조, 이 리뷰에선 생략함)   
        (오른쪽 끝 수식에서 i가 누락됨)
    - $p=\infty$일 경우: 
        ![fo5.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgSH1mgim6MKS5M1u?embed=1&width=877&height=68)
        - 특정 타켓 샘플과 높은 유사도를 가질 경우 선택
        - 전체 샘플 간 유사도를 기반으로 선택하는 것으로, 각 샘플 별 가장 가까운 샘플을 선택하는 Nearest Neighbor 방식과는 다르며 우수함(성능비교 appendix참조)
        ![t8.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgSdIcnBo1oCDEbK_?embed=1&width=933&height=238)

### Choice of Target Data.
- NormSim$_p$(1N-1k): ImageNet-1k의 학습 데이터
- NormSim$_p$(Target): 24개의 downstream task의 학습 데이터

### Necessity of using vision-only information
- OPenAI CLIP이 다수의 짧은 캡션으로 학습이 되어 언어 임베딩이 이미지 임베딩보다 약함(부족함)
- 이로 인해 언어 부분은 이미지보다 downstream task의 분포를 잘 설명하지 못함(Appendix참조)
-> 이미지만 사용
    ![t9.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgSaIBhl-XA0K_Sm7?embed=1&width=943&height=393)

### Generality of NormSim in choosing teacher model.
- Normsim 측정에서 이미지 임베딩만 사용하기 때문에, CLIP을 사용하는 것은 불필요할 수 있음
- 사전학습된 ResNet-50과 같은 이미지 표현이 제공되면, 더 일반적인 평가지표로 활용될 수 있음(별도의 실험은 없음)

### Theoretical justification.
- 기존 방법들이 데이터 다양성을 강제하는 반면, 저자의 방식은 다양성을 직접 고려하지 않고 유사성을 극대화함.
- $p = 2$의 경우, NormSim$_2$를 극대화하는 것이 선형 모델 하에서 최적임을 증명(appendix참조, 리뷰에선 생략).

### Using proxy when downstream $X_{target}$ is inaccessible.
- 사전 학습 데이터셋만 사용할 때도 사용할 수 있음을 보여줌
- $S_i$가 현재 선택된 서브셋이라고 가정, 이를 프록시 "target"세트로 구성. 아래 조건을 만족하는 더 작은 다음 배치$S_{i+1}$를 구성   
$\text{argmax}\_{S_{i+1}\subset S\_{i}} \sum\_{x \in S}\text{NormSim}\_2(S\_i, x)$    
이를 $N$크기의 서브셋이 될 때 까지 반복함

### 정리
![fig3.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgRqJxRpQ_mYimUGG?embed=1&width=922&height=715)
- 두 필터링 방식을 같이 사용하였을 때, 높은 퀄리티와 downstream task에서의 적합성을 모두 챙길 수 있음
- Type4의 경우 negCLIPLoss에서는 높은 점수가 기록되었으나, OCR 내용이 지배적으로 Downstream task에서 부적합함.(negCLIPLoss만으론 OCR을 필터링 하기 어려움)


# 4. Experimental Results
negCLIPLoss와 NormSim의 성능을 평가하며, 다음 질문에 답하는 것을 목표로 함:   
- Q1: 고정된 CLIP 교사 모델이 주어졌을 때, 우리의 방법이 CLIP 임베딩을 더 효과적으로 활용하여 데이터 필터링을 할 수 있는가?
- Q2: 우리의 방법이 다양한 아키텍처 또는 서로 다른 사전 훈련 데이터셋을 가진 다양한 CLIP 교사 모델에 적용될 수 있는가?
- Q3: 우리의 방법이 외부 모델이나 멀티모달 데이터셋을 활용하는 다른 주요 방식과 비교했을 때 어떤 성능을 보이는가? 추가로, 우리의 방법이 이러한 방법과 호환되어 그 효과를 향상시킬 수 있는가?

## 4.1. Setup
### Training configuration.
- DataComp의 중간규모 구성을 사용함(DataComp-medium): 필터링 할 1억 2800만개의(일부 링크가 소실돼서 1억 1000만개) 저품질 웹에서 수집한 이미지-텍스트 쌍으로 구성된 데이터셋임
- 데이터 필터링을 통해 서브셋을 얻어, CLIP-B/32를 학습함

### Evaluation.
- 이미지 분류와 이미지 retrieval 등 38개의 downstream 데이터셋에서 평가

### Teacher model architecture.
-  OpenAI ViT-L/14 및 ViT-B/32 아키텍처 사용, DFN-P 모델을 사용

## 4.2. Baselines
- (D1): OpenAI의 CLIP만 사용하여 임베딩 활용 전략을 최적화하는 방법
- (D2): 외부 데이터를 기반으로 더 발전된 CLIP 임베딩 모델을 학습하는 방법
- (D3): 데이터 선택을 지원하기 위해 non-CLIP 외부 모델을 활용하는 방법

이때 D2와 D3는 D1의 전략을 포함할 수 있음

## 4.3. Main Results and Discussions
### 4.3.1. Comparision on D1 Category (Q1)
![t2.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgSNASChd01UpEtP_?embed=1&width=925&height=686)
- negCLIPLoss가 CLIPScore보다 모든 평가지표에서 우수한 성능을 보임
- NormSim$_2$를 negCLIPLoss와 함께 사용하면 평균 38개 다운스트림 태스크에서 1.9% 향상
- $\text{negCLIPLoss} \cap \text{NormSim}_\infty(\text{Target})$가 ImageNet-1k에서 5.3%, 평균 38개 다운스트림 태스크에서 2.8% 개선하여 D3의 방식을 제외하면 최고 성능을 달성
- ImageNet-1k를 타겟 데이터로 사용할 떄는 Norm 선택이 크게 영향을 미치지 않음

### 4.3.2. Try Other Teacher Models (Q2)
- OpenAI CLIP-L/14 -> OpenAI CLIP-B/32 or DFN-P
    ![t1.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgSKV3HbYNPGHBk0t?embed=1&width=300&height=764)
    - 일관된 성능 개선을 보임

- 임베딩의 중요성:
    - NormSim에 필요한 임베딩은 하위 작업 성능이 좋아야 함
    - OpenAI CLIP-B/32는 더 나은 성능을 보이는 반면, DFN-P는 성능이 저하됨

- DFN-P의 신뢰성:
    - DFN-P에서 얻은 임베딩은 신뢰할 수 없으며, 유사성 계산에 부정확성을 초래할 가능성이 있음
    - DFN-P를 사용하여 negCLIPLoss를 평가하되 NormSim 계산에 OpenAI CLIP-B/32를 사용하면 성능 개선.

- 38개의 Task에서 평균 성능이 최고의 DFN( HQITP-350M에서 학습된)보다 높음.

### 4.3.3 Comparison with D2 & D3 Categories (Q3)
![t3.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgST2p2edGV-uUQBT?embed=1&width=918&height=490)
![t4.PNG](https://1drv.ms/i/s!AvuRV8CuQlavgSW6H5j23RaON0uI?embed=1&width=390&height=314)
- 4.2장의 모든 D2, D3 방식과 제안 방식(최상의 결과)를 비교
- 제안 방식이 외부 모델이나 추가 데이터를 사용하지 않고도 DFN $\cup$ HYPE를 제외한 모든 방법보다 우수함
- 제안 방식이 다른 D2, D3 방식과 호환 될 수 있음

# 5. Conclusion and Limitation
- Contribution
    - negCLIPLoss와 NormSim을 통해 외부 자원에 의존하지 않고 데이터 선택을 향상
    - negCLIPLoss는 CLIPScore보다 더 정확한 품질 평가지표
    - NormSim을 통해 다운스트림 태스크와 유사성을 측정하여 효과적인 데이터 선택이 가능

- Limitations
    - DataComp의 더 큰 규모의 사전 학습 데이터셋을 포함하지 않았음(DataComp-medium이 CLIP 평가에 가장 일반적으로 사용됨)


# Review
- 제안 방식은 비교적 간단하지만, 많은 양의 실험과 뒷받침하는 이론적 근거가 탄탄하게 제시된 논문
- 대규모 사전학습 뿐만 아니라, 다른 데이터 선택이 필요한 Task들에 충분히 활용이 가능한 방식임   
(동영상의 프레임 선택에도 사용이 가능할지도)
- 현재 생성 모델의 평가에 CLIPScore가 많이 사용되고 있는데, negCLIPLoss로 대체가 될 것인지 기대됨

