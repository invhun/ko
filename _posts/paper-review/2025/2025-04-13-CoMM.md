---
title: "[논문 리뷰] What to Align in Multimodal Contrastive Learning"
author: invhun
date: 2025-04-01 16:00:00 +0900
categories: [Paper Review, Multimodal Learning]
tags: [Recommendation System, Multimodal Learning, Matryoshka Learning, Embedding Compression]
math: true
pin: false
---

> TWhat to Align in Multimodal Contrastive Learning   
> ICLR 2025   
> **Benoit Dufumier**, **aviera Castillo Navarro**, Devis Tuia, Dong Wang, Jean-Philippe Thiran   
> EPFL, NeuroSpin-CFA, CEDRIC-CNAM, Radiology Department CHUV    
> [[paper](https://openreview.net/pdf?id=Pe3AxLq6Wf)], [[github](https://github.com/Duplums/CoMM)]


Benoit Dufumier1,2∗ Javiera Castillo Navarro1,3∗ Devis Tuia1 Jean-Philippe Thiran1,4
1 EPFL 2 NeuroSpin, CEA 3 CEDRIC, CNAM 4 Radiology Department, CHUV

# 1. Abstract & Introduction

### 기존 연구 문제점
- 인간 경험은 본질적으로 다중 모달적이며, 여러 감각 신호가 결합되어 정보를 제공. 하지만 기계학습은 주로 단일 모달에 집중해왔음
- 최근에는 self-supervised 방식의 대조학습을 통해 다중 모달 표현 학습을 진행하였으나, 이러한 방법은 모달 간 중복 정보에 의존하기 때문에, 한계가 존재함
- 모달 간 상호작용은 Task에 따라 Redundancy (R), Uniqueness (U), Synergy (S)로 나뉘며, Task에 구애받지 않는 다중모달 표현을 학습하는 것은 도전적임
    - R: 두 모달 중 어느 하나라도 task를 수행할 수 있는 경우
    - U: task를 해결하는데 필요한 모든 정보를 오직 하나의 모달만 포함할 경우
    - S: 두 모달이 보완적인 정보를 가지고 있으며, 동시에 task를 수행하는 데 필요할 경우

### 제안 방법
![fig1]
- Contrastive MultiModal self-supervised pre-trained method (CoMM)을 제안하여, 모달 간의 통신을 단일 다중 모달 공간에서 가능하도록 함
- CoMM은 global workspace theory에 기반하며, 상호 정보를 극대화하여 모달 특징을 정렬하고, 다중 모달 상호작용을 모델링
- 여러 실제 데이터셋에서 SOTA를 달성하였으며, 다양한 도메인에 적용 가능함


# 2. Quqntifiying Multimodal Interactions

### Problem setup
- $$ X_1, X_2, \ldots, X_n $$ 은 $$n$$개의 데이터 모달을 나타내며, 목표는 Y에 대한 좋은 표현 $$Z = f(X)$$ 를 학습하는 것. 이론적 분석을 위해 $$n=2$$로 설정
- 결과적으로 좋은 표현 Z를 얻기 위해서는 결합 변수 X와 task Y간의 정보를 모델링 해야함
    - $$I(X; Y) = I(X_1, X_2; Y)$$

### Partial information decomposition (PID)
- PID에 따르면 다변량 상호작용 정보 $$I(X_1, X_2; Y)$$은 세 가지 형태의 상호작용으로 분해 됨
    - (1) Uniqueness: Y를 단 하나의 모달만으로 완료할 수 있을 때: $$U_2 or U_2$$는 $$X_1, X_2$$가 Task관련 모든 정보를 포함하는 경우를 나타냄
    - (2) Redundacny: $$X_1과 X_2$$가 Y에 대한 동일한 정보를 포함할 때: $$R$$은 중복 또는 공유 정보를 나타냄
    - (3) Synergy: $$S$$로 표시되며, $$X_1과 X_2$$가 동시에 존재할 때만 나타나며, 서로 다른 보완적 작업 관련 정보를 제공함
- 따라서  $$(X_1, X_2)$$가 Y에 대해 가지는 정보는 네 가지 용어의 기여로 표현될 수 있다:
    - $$I(X_1, X_2; Y) = R + S + U_1 + U_2$$
- 이에 대해 "chain rule of mutual information"을 적용하면, 아래와 같은 consistency equation을 얻을 수 있음
    - $$I(X_1; Y) = R + U_1, \quad I(X_2; Y) = R + U_2, \quad I(X_1; X_2; Y) = R - S$$
- CLIP과 같은 기존 대조학습을 사용하는 방식은 $$ I(X_1; X_2) $$의 추정치로 $$I(X_1, X_2; Y)$$를 근사하는 방법이기 때문에, "Task 관련 모든 정보가 모달 간에 공유되고 중복되어 있다는 전제 하에 효과적으로 작동함"

### Assumption 1 (Minimal label-preserving multimodal augmentations)
- $$T^{\star}$$라는 멀티모달 augmentation set이 존재한다고 가정하면 $$I(X, X') = I(X, Y)$$를 만족
- 즉 augment된 데이터 X'가 원래 데이터 X와 작업 Y에 대한 정보를 동일하게 보존하여야함
- 예를 들어, "노란 꽃"이라는 캡션을 가진 꽃 이미지에 색상 변화를 적용하면, 모델이 색상 중복성 보다는 다른 상호작용(U or S)에 집중할 수 있게됨

# 3. CoMM: Contrastve Multimodal Learning

## 3.1. Towards Effective Multimodal Representations
- task에 구애받지 않는 표현 Z를 얻기 윟해, R, U, S를 모두 포착하도록 신경망 $$f_{\theta}$$를 설계
- $$X' = t(X)$$로 정의, 여기서 $$t \in T$$는 X의 확률적 매핑(다중 모달 증강)을 의미
- 이때 $$Z′θ = fθ(X′).$$로 augmented된 X'로 부터 생성된 표현을 뜻함

![fo3]
- 이에 따라 위와 같은 inequality 수식이 성립함
- $$Z'\theta$$가 $$X$$로 부터 유도되기 때문에, 원본 데이터를 온전히 보존하지 않을 수 있고, 이에 따라 $$Z'\theta$$와 $$Z\theta$$ 간의 상호정보는 $$X$$와 $$Z'\theta$$ 간의 상호정보보다 작거나 같아야 함
- 같은 이치로 우측 식도 성립함
- 이를 활용하여, 아래 lemma들을 증명

![lemma2]
- 충분한 표현럭이 있는 네트워크를 가정할 경우, 전자를 극대화하면, 최적에서 위의 식이 성립함
- 즉 네트워크가 두 표현 간의 관계를 학습하여 최적의 상태에 도달하였을 때, 두 표현의 상호정보가 동일함

![lemma3]
- 같은 방식으로 증명이 가능하며, 최적의 표현 $$Z'_\theta^\star$$가 변형된 입력 X의 Y에 관한 정보를 유지하고 있음을 의미함

![results]
- 이론적 기반을 통해, CoMM이 다중 모달 상호작용을 효과적으로 학습할 수 있음을 보여줌

## 3.2. Multimodal architecture
![fig2]
- Encoder: 모달별 별도의 인코더로 구성
- Latent Converters: 선형 모듈로, 인코더에서 생성된 특징을 모달별 임베딩 시퀀스로 변환, 이를 concat
- Transformer block: MHSA로 모달별 임베딩을 융합 -> 다중 모달 임베딩 Z를 구성

## 3.3 Training
![fig3]


![fo6]
![fo7]
- 각 모달의 정보를 개별적으로 활용하기 위해서, 마스킹을 적용
- n+2개의 임베딩에 대해, 총 2n+1개의 상호정보학몽이 최적화됨
- 그림과 같이 두가지 방식으로 구분하여 각각 infoNCE로스를 적용

### Ai inference
- 추론 단계에서는 증강을 적용하지 않음


# 4, Experiments

## 4.1. Controlled Experiments on the Bimodal Trifeature Dataset

- CoMM의 R, U, S 정보의 학습을 평가하기 위해, Trifeature 기반의 합성 데이터셋을 설계
    - 첫번쨰 모달: Trifeature 데이터셋을 생성하여 10개의 형태, 10개의 텍스처, 10개의 색상으로 구성된 이미지(총 1,000가지 조합)를 포함하고, 각 이미지는 회전 및 이동을 통해 세번 증강됨
    - 두번째 모달: 동일한 데이터셋에서 각 이미지에 대해 두번째 이미지를 쌍으로 만들어, 두 모달 간의 R, U S 특성을 제어
- 모든 실험에서 AlexNet(why??)를 모달별 인코더로 사용

![fig4]
- Experiment 1 - Modeling shared and unique interactions
    - "shape"를 공유 특징, "Texture"를 고유 특징으로 선택하여 같은 shape의 쌍으로만 이미지를 선택
    - shape(or texture) Linear proving 정확도를 측정하여, 모델의 잠재표현에서 R(or U)가 포착되었는지를 평가, 이때 10%가 random한 성능에 가까움

- Experiment 2 – Modeling synergy
    - 실험1에서 독립적이였던 특징 간의 Synergy를 도입하기 위해 "texture"와 "color"간의 매핑을 정의
    - 주어진 이미지 쌍이 매핑을 준수하는지를 평가함, 랜덤한 성능은 50%

- 그림 4는 기존 대조방식이 R은 완벽하게 포착하지만, U와 S는 전혀 고려하지 못함을 보여줌

## 4.2 Experiments with 2 Modalities on Real-world Datasets
![t1]
- 앞선 실험은 통제된 합성 데이터셋의 평가이며, 여기선 실제 벤치마크에서의 평가를 수행
- 가장 유사한 방식인 FactorCL과 fair하게 비교하였을 때, 대부분의 데이터셋에서 띄어난 성능을 보임
- 또한 다양한 데이터 도메인에 적용가능한 versatility를 갖추고 있음을 보여줌
    - V&T EE(Robot), MMIC(table, sequence), MOSI(vision, audio), URFUNNY(vision, audio, language), MUsTARD(vision, audio, language)

![t2]
- MM-IMDb는 영화장르예측 데이터셋으로 vision(영화 포스터), text(영화줄거리 설명)으로 구성됨
- 다중 레이블 분류 task로, 장르 예측은 영화 포스터나, 줄거리 설명 만으로 정확하게 예측할 수 없으며, 이에 따라 Synergy평가에 유용
- CoMM이 뛰어난 성능을 보이며 Synergy 상호작용이 중요한 역할을 하는 것을 보여줌
- CLIP의 경우 fine-tuning했을 떄, 오히려 성능이 감소하였는데, 이는 "R"정보 학습이 장르 예측에 도움이 되지 않음을 시사함

## 4.3. Experiments with 3 Modalities on Real-world Datasets

![t3]
- 모달리티를 추가하였을 때도, 일관된 성능 향상을 보여주었음

# 5. Ablation Studies

### Loss function
![fig5]
- 식 7의 전체 식을 최적화 하는것이 필요함을 검증
- 후자만 최적화할 경우, R과 U는 개선되지만, S는 실패함
- 반면 전자만 최적화 할 경우, 모든 정보를 학습할 수 있지만, 매우 느리게 진행이 됨
    - S의 경우, 모달리티 특성을 학습 후, 상호작용을 학습하기 때문


### Fusion module
![t4]
- Latent Converter와 단순 Lineary layer와의 비교를 수행
- 단순 선형 상호작용으로는 모델의 표현력을 제한할 수 있음

### Data Augmentation
![t5]
- 강력한 data augmentation이 다중 모달 상호작용을 학습하는데 필수적임을 보여줌







