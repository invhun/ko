---
title: "[논문 리뷰] DrVideo: Document Retrieval Based Long Video Understanding"
author: invhun
date: 2025-03-24 15:00:00 +0900
categories: [Paper Review, Multimodal Learning]
tags: [Text-Video, Video Question Answering, Video Undersanding, Long Video]
math: true
pin: false
---

> DrVideo: Document Retrieval Based Long Video Understanding   
> CVPR 2025   
> **Ziyu Ma**, **Chenhui Gou**, Hengcan Shi, Bin Sun, Shutao Li, Hamid Rezatofighi, Jianfei Cai   
> College of Electrical and Information Engineering, Data Science & AI Department, Faculty of IT, Monash University    
> [[paper](https://arxiv.org/abs/2406.12846)]

# 1. Abstract & Introduction

### 기존 연구 문제점
- Video LM의 한계
    - 비디오 전체를 Video-LMs의 입력으로 처리할 수 없음(LLaVa-NeXT-Video의 최대 길이는 8192)
    - 균일 혹은 무작위 프레임 샘플링은 중요한 정보가 손실 될 수 있음
    - 자연어와 시각 토큰의 단순 연결은 LLM의 추론 능력을 제대로 활용하지 못함
- 기존 연구 (LLM, Agent 활용) 방법의 문제점
    - Coarse한 방법으로 비디오 전체에 대한 이해가 부족, LLM의 사전지식에 지나치게 의존함
    - 비디오를 캡션으로 변환하여 Long 추론을 다루지만, 중요한 정보의 손실이 여전히 발생함. e.g.)”여자가 거울을 보고 있다”는 캡션은 “여자가 비디오에서 거울 앞에 서있을 때 무엇을 입고 있는가”라는 질문에 대한 답변에 기여하지 못하여, LLM은 무작위 추측만 할 수 있음

### 제안 방법
- DrVideo를 제안하여, 긴 비디오 이해 작업을 긴 문서 검색 및 이해작업으로 변환하여, LLM을 효과적으로 활용하는 방식을 최초로 제안
이전 연구와 달리, 잠재적인 누락 정보를 동적으로 찾아내고 언어 공간 내에서 정보를 보강하는 새로운 검색 모듈과 다단계 에이전트 상호작용 루프를 제안

# 3. Methodology
![fig1](https://1drv.ms/i/c/af5642aec05791fb/IQSKFklMhS6dRLDcXg5KlO2mARRrtkm5Ee--Rq-6DtHUyUY?width=1335&height=535)
- Initial Stage
    - Video Document Conversion Module: 긴 비디오를 초기 문서로 변환
    - Document Retrieval Module: 질문과 초기문서 간의 유사성을 계산하여 K개의 핵심 프레임 식별
- Multi-Stage Agent Interaction Loop
    - Document Augmentation: 핵심 프레임 정보를 추가하여 업데이트된 비디오 문서 생성
    - Planning Agent: 질문에 대한 현재 정보의 충분성을 판단, 충분할 경우, Answering Module로 부족할 경우 Interaction Agent로
    - Interaction Agent: 정보가 부족할 경우, 누락된 핵심 프레임을 추가

## 3.1. Video-Document Conversion Module
![fo1](https://1drv.ms/i/c/af5642aec05791fb/IQQ-wwKkgbHlTrGleise_ryDATqLA3HPgNNp7KxiG6KD6I4?width=584&height=52)
- 각 비디오 프레임을 LLaVA-NeXT를 사용하여 짧은 캡션으로 변환 -> 초기 비디오 문서로 구성
- prompt: describe the picture in no more than 50 words.

## 3.2. Document (Frames) Retrieval Module
![fo2](https://1drv.ms/i/c/af5642aec05791fb/IQT151TKtcH-TYnipbJld22vAQbSeeiAEAsPz-Np7WCbZwg?width=550&height=51)
- OpenAI Text-Embedding 3를 사용하여, 질문과 전체 문서 간의 코사인 유사도를 기반으로 상위 K개의 프레임을 검색

## 3.3. Document Augmentation Module
![fo3](https://1drv.ms/i/c/af5642aec05791fb/IQSHB9x6taimRLqKlYJKptcqASED64z1tc7NwOSgd84J7s0?width=595&height=90)
- 초기 prompt(LLaVA-NeXT): If there are factual errors in the question, provide a precise description of the image; if not, proceed to answer the question: {Q}
- 질문의 오류 여부에 따라 이미지를 설명하거나, 질문에 답변하도록 지시
- 업데이트된 비디오 문서는 보강된 설명을 포함하여 재구성, K는 전체 프레임 수에 비해 적기 때문에 문서의 전체 길이에 큰 영향을 미치지 않음

## 3.4. Multi-Stage Agent Interaction Loop

### Planning Agent
- 질문(Q)와 업데이트 된 문서 AD_i, 그리고 모든 이전 단계의 분석 기록 H를 바탕으로, 현재 단계의 문서가 답변을 생성하기에 충분한지를 판단
- 충분하다면 Answering module로, 불충분하다면 불충분한 이유에 대한 분석을 제공하고 H를 업데이트, 이를 Interaction Agent에 전달
![planning_prompt]

### Interaction Agent
![fo4](https://1drv.ms/i/c/af5642aec05791fb/IQSn3k6fjalHT6LpZVMg9SyZAQPXMAf7B_iH7kHExdNo9E0?width=603&height=51)
- 현재 비디오 문서와 업데이트된 H가 주어지면, 질문 Q에 답하는데 중요한 세부정보가 누락된 N개의 프레임을 찾음
- 각 프레임애 대한 보강 정보 유형을 결정하기 위해 작업 특화 프롬프트를 제공
    - 이미지가 주어졌을 때, 이미지에 대한 자세한 설명을 얻기 (이미지 캡션)
    - 이미지가 주어졌을 때, 질문에 대한 응답을 얻기 (시각적 질문 답변)
- 문서 보강 모듈과 상호작용하여, 현재 비디오 문서 업데이트
![interaction_prompt]

## 3.5. Answering Module
- 최종 비디오 문서를 기반으로 Chain-of-Thought(CoT) 접근 방식을 사용하여, 예측을 제공하는 에이전트를 사용
- 답변 모듈은 답변과, 신뢰 점수, 답변의 근거를 출력하여 예측 정확도 향상과, 의사 결정 과정의 투명성과 실명 가능성을 보장
![answer_prompt]

# 4. Experiments

### Dataset
- EgoSchema: 5000개의 3분 길이 ego video, 5000개의 다중선택질문
- MovieChat-1K: 영화 및 TV쇼의 1000개의 비디오, 각 비디오는 10분길이
- Video-MME: 비디오 길이(30-60분) 평균 44분

### Implementation Details
- EgoScheme는 비디오-문서 변환에 LaViLa 사용, 나머지는 LLaVa-NeXT 사용
- 비교 실험에서는 GPT-4를 에이전트로 사용, Video-MME 평가는 DeepSeek V2.5를 사용, Ablation Study에는 GPT-3.5를 사용

## 4.3. Main Results
![t1](https://1drv.ms/i/c/af5642aec05791fb/IQRAYeY_sQIDTaE_D-qDMGqfASvkp2oU2NBEzsW3EWXUZOQ?width=641&height=481)
![t2](https://1drv.ms/i/c/af5642aec05791fb/IQRD9FZ5pceUSaVy2_p8TcAQAb0qQSc1TU7bvtA1v4AEY20?width=649&height=556)
![t3](https://1drv.ms/i/c/af5642aec05791fb/IQR9DUgqIIukTKsz8cfCx13JAVixlXfnOyV5F5eUsQXHBGA?width=642&height=826)

## 4.4. Ablation Studies
![t4](https://1drv.ms/i/c/af5642aec05791fb/IQS7aHWrS4lJT7f_Vyz9m3ylAfgGWaFCYbocAGviW_-uy0s?width=653&height=335)
- MSAIL의 효과가 가장 두드러짐

![t5](https://1drv.ms/i/c/af5642aec05791fb/IQTcJ-yF2AzeQoawzPYz2EMGAZh9LnakNlx_4OTD4YbivrA?width=622&height=266)
- 다양한 유형의 정보로 보강의 필요성, 더 많은 프레임은 노이즈가 될 수 있음

![fig3](https://1drv.ms/i/c/af5642aec05791fb/IQSIi3AS654xToOwm73laZLlAZFfLj11x-0Rwn0Jl6NUoQA?width=645&height=468)
- 반복 횟수 2회에서 최고 성능 달성하며, VideoAgent보다 일관적으로 높은 성능 

![t6](https://1drv.ms/i/c/af5642aec05791fb/IQRFPri_WYojTKPeOEi66W5DAXYDlr7m-UUyp2asU_FaVmM?width=572&height=207)
- 모션 정보를 캡처하기 위해서는 프레임 기반 보다 비디오 클립 기반이 우수함

![t7](https://1drv.ms/i/c/af5642aec05791fb/IQQv8_ln3j3xSLreCQhAYOepAaF9yNpN0FyfhgJZQdA0XS4?width=582&height=211)
- LLM의 능력이 증가함에 따라 성능 향상, 더 나은 모델과 결합하여 더 개선될 여지가 있음

![fig4](https://1drv.ms/i/c/af5642aec05791fb/IQTM0Qnt-AKUQalOsQzoo8fzAZAiUxY3nEuPOhTX_AU1pVQ?width=1177&height=885)

# Review
- 일종의 Video Question Answering 논문으로, LLM을 안 쓰는건 이제 찾기가 어려운듯 함
- LLM을 Agent로 쓰는 논문들이 증가함, 이러한 Long video(평균 44분)를 LLM이 아닌 단순 모델링으로 커버를 하는게 가능할지? 어쩔 수 없는 현상이지 않을지
