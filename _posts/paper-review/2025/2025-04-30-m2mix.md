---
title: "[논문 리뷰] Geodesic Multi-Modal Mixup for Robust Fine-Tuning"
author: invhun
date: 2025-04-30 01:10:00 +0900
categories: [Paper Review, Multimodal Learning]
tags: [Text-Video, Video Question Answering, Multimodal Learning, Representation Learning]
math: true
pin: false
---

> Geodesic Multi-Modal Mixup for Robust Fine-Tuning   
> Neurips 2023   
> **Changdae Oh**, **Junhyuk So**, Hoyoon Byun, YongTaek Lim, Minchul Shin, Jong-June Jeon, Kyungwoo Song    
> University of Seoul, POSTECH, KAIST, Yonsei University    
> [[paper](https://openreview.net/pdf?id=iAAXq60Bw1)] [[github](https://github.com/changdaeoh/multimodal-mixup)]

# 1. Abstract & Introduction

### 기존 연구 문제점
- 대규모 사전학습 모델이 foundation 모델로 각광받고 있음
- CLIP등 멀티모달 대조학습 모델은 이미지-텍스트 임베딩을 명시적으로 정렬을 수행하지만, 실제로는 모달리티별 분리된 이분화 임베딩 공간(bipartite embedding space)를 가짐. 즉 모달리티 갭이 존재함
- 이 구조로 인해 uniformity와 alignment가 모두 낮으며 이는 fine-tuning 후에도 개선되지 않는데, 낮은 uniformity-alignment는 임베딩의 transferability와 robustness를 제한함.
- 온도 변수 조절로 모달 갭 분리 문제 완화가 가능하지만, 이는 downstream task별 manual tuning이 요구되며, uniformity-alignment 사이의 trade-off가 발생함.

### 제안 방법
![fig1]
- geodesic multimodal mixup($$m^2$$-Mix)를 제안
    - 이미지와 텍스트 임베딩을 혼합해 하이퍼스피어 상의 hard negative 샘플을 생성 -> 대조손실에 활용하여 alignment와 uniformity를 개선
    - 도메인 간 샘플을 보간하여, 사전학습된 CLIP이 활용하지 못한 임베딩 공간을 확장함 -> uniformity 증가
    - 임베딩 공간에서 virtual augmented sample을 생성하여, out of distribution, in-distribution 샘플 모두에서 나은 성능을 보임


# 2. Related work

### Mixup
- 데이터 증강 기반 중 하나로, 두개의 서로 다른 샘플을 선형적으로 혼합하여 새로운 학습 샘플을 생성하는 것
    -> 모델이 '중간'데이터를 학습하게 하여 경계를 부드럽게 만들고, 과적합을 방지, 일반화 성능을 높일 수 있음
- 멀티모달 학습에 적용한 연구는 speech와 text 특징을 혼합해 공유 임베딩 공간을 정렬하는 STEMM이 있지만, 아키텍처 특화 설계로 일반적인 용도로 사용하기 어려움
- 저자는 여러 멀티모달 표현 학습에 적용 가능한 $$m^2$$-Mix를 제안함
# 3.  Observations and Problem Define
![fig2]
![eq2-3]
- 그림2의 왼쪽은 DOSNES를 통한 CLIP 임베딩 공간 시각화, 중앙은 zero-shot, 일반 Fine-tuning, 제안 방식으로 fine-tuning 후 uniformity와 alignment결과를 보여줌. (저자는 기존 alignment 측정 방식에 가장 어려운 음의 쌍의 거리를 고려할 수 있도록 뒤에 항을 추가하였음)
- 사전학습된 CLIP과 일반 Fine-tuning의 경우 낮은 uniformity와 alignment를 보여서 전이성과 강건성이 제한되며, 제안 방법으론 개선됨을 확인
- retrieval, 분포 내/외 classification, captioning, embedding arithmetic (SIMAT), uncertainty calibration (ECE)와 같은 다양한 다운스트림 작업에서 일관된 성능 향상이 있음을 보여줌

# 4. Methodology
## 4.1 Understanding Geodesic Multi-Modal Mixup

![eq4]
- 강건성 문제 해결을 위해 hard negative sample을 활용하여 CLIP을 fine-tuning을 수행
- 이미지 임베딩과 텍스트 임베딩을 혼합하여 하드 네거티브를 생성하는데, 기존 임베딩들은 L2 정규화로 인해 하이퍼스피어 상에 위치하므로, 이 혼합 하드네거티브 샘플도 하이퍼스피어 상에 위치하도록 설계
- Geodesic Mixup을 통해 이를 가능하게 함
- $$\lambda$$는 베타 분포의 하이퍼파라미터 $$\alpha$$로 조정되며, 이 방식은 두 데이터 포인트의 최단경로(geodesic path)상에서 보간되기 떄문에, 혼합 샘플이 하이퍼 스피어상에 위치하도록 보장함

![eq5]
- 기존 로스의 분자는 유지하고, 분모만 수정 -> 기존 네거티브 샘플을 혼합 하드네거티브 샘플로 대체

### Theoretical Analysis
- CLIP이 임베딩 갭이 있으며, fine-tuning에서도 갭을 줄일 수 없는 이유에 대해서 저자는 아래와 같이 주장함
    - Hard negative sample의 부족
    - Loss에서 학습가능한 tempereature가 0.01로 수렴하였기 때문
    - temperature가 0에 근접하면, 대조손실은 마진이 0인 triplet loss와 동일하게 됨. 즉, 긍정 샘플간의 유사도가 가장 가까운 부정 샘플의 유사도보다 크면(로스가 0이됨), 임베딩들을 추가로 당긱나 밀어낼 동기가 없음
    - CLIP의 이분적인 임베딩 공간 때문에, 모델이 더 큰 uniformity와 alignment를 추구하도록 동기를 부여하기 위한 hard negative sample이 부족함

![theorem4.1]
- theorem 4.1은 충분히 큰 k에 대해서 기존 샘플과 혼합된 샘플 (하드네거티브 샘플) 쌍 간의 KL-divergence가 기존 쌍간의 KL-divergence보다 작음을 보여줌

![proposition4.2]
- 충분히 큰 M에 대해 로스의 온도가 0에 수렴한다면, infoNCE loss는 마진이 0인 triplet loss로 즉 negative alignment로, 제안하는 로스는 마진이 0인 negative uniformity로 수렴함

- 결론적으로 hard negative sample을 생성함으로써 모달리티 갭의 유무와 상관없이 모델이 alignment를 더 강하게 강화하도록 동기를 부여하게 됨
- 또한 명제 4.2에 따라 uniformity 또한 명시적으로 증가하는 방향으로 학습이 됨.

## 4.2 Uni-Modal Mixup for CLIP

![fig3]
- 기존 단일 모달에서의 mixup 방식을 확장하여 멀티모달에서 세 가지 단일 모달 mix up을 추가로 제안

![eq6]
- V-Mix의 경우 원래 이미지 임베딩과 배치 내 역순의 이미지 임베딩을 $$\lambda$$와 $$1-\lambda$$로 혼합을 수행
    -> $$m_{\lambda}(I_{i}, I_{i'})$$
- $$T_i$$는 원래 이미지 임베딩의 페어 레이블, $$T_i'$$는 역순 이미지 임베딩의 페어 레이블을 의미함
- 로스 계산도 $$lambda$$ 가중치를 사용하여 각각 계산 후 합침

![eq7]
- L-Mix도 동일한 방식으로 수행 후, V-Mix의 로스와 합친 것이 최종 uni-Mix 로스

![eq8]
- VL-Mix는 이미지와 텍스트 임베딩을 동시에 독립적으로 혼합을 수행
- 혼합 쌍 간 유사도 계산

![eq9]
- 이 모든 로스를 다 합친것을 $$m^3-Mix$$로 명명
- 로스 가중치는 task별로 다르게 설정 (기본 로스는 1,0으로 고정 후, 다른 로스들은 {0.0, 0.01, 0.1, 0.2, 0.3, 0.5} 범위에서 탐색)

# 5. Results
## 5.1. Generated Samples by $$m^2-Mix$$

![fig4]
- 혼합 임베딩을 시각화는 불가능하므로, 이와 가장 유사한 이미지를 시각화함
    -> 원본 이미지 임베딩에 부족했던 특징을 포함함
- 학습 초기부터 최종 epoch까지 원본 샘플보다 지속적으로 더 강한 hard negative sample을 생성함을 보여줌

## 5.2 Cross-Modal Retrieval with CLIP

![t1]
- 제안 방법과 단순 fine-tuning방법, 그리고 기존 Mixup 방식을 비교하였을 때, 제안 방법이 가장 우수한 성능을 달성
- 아쉬운 점은 제안 방식 중 $$m^2-Mix$$가 uniformity와 aligmnent를 모두 증가시킴을 주장하였는데, 높은 온도 변수 0.1에 대한 결과가 부재함. 즉 높은 온도 변수에서 비교적 alignment가 유지됨을 retrieval 결과로 보여주었어야 했다고 생각함

[fig5]
[t3]
- Expected Calibration Error를 측정했을 때, 제안 방식의 오차가 가장 적으며, 온도변화에 강건함을 보여주었음

# 5.3 Cross-Modal Retrieval with Uni-Modal Pre-Trained Models

![t2]
- 독립적으로 사전학습된 단일 모달모델을 멀티모달 임베딩 공간으로 fine-tuning 하는 방식에 대해 실험을 수행
- 제안 방식이 가장 우수한 성능을 달성하며, 독립적으로 학습된 모델을 fine-tuning할 때 적합함을 보여줌

# 5.4 Few-Shot Adaptation and Robustness on Distribution Shift

- 다른 다운스트림 task도 유사한 결과를 보임으로 생략

![t6]
- mixup을 수행할 때, Manual L2-Norm linear mixup 대비 제안 방식이 우수한 성능을 보임
- 즉 하이퍼스피어기반 학습에서 geodesic 접근이 더 적합함을 시사함

# 5.6 Multi-Modal Embedding Arithmetic
![fig7]
![t8]
- 잘 학습된 멀티모달 임베딩은 단어 벡터와 유사하게 구조적 관계를 반영할 것으로 기대되기 때문에, 이를 검증하기 위해 텍스트 기반 이미지 변환 성능을 평가함
- 원본 텍스트에서 특정 단어를 변경한 벡터와 원본 벡터의 차이에 가중치를 곱한 후, 이를 이미지 임베딩에 더해서 새로운 임베딩을 생성 -> 이후 이와 가장 유사한 이미지를 가져옴
- 제안 방식이 2개의 데이터셋에서 가장 우수함을 보여줌

# 5.7 Multi-Modal Mixup on a State-of-the-Art Vision-Language Model

![t9]
![t10]
- CLIP이 아닌 최신 대규모 비전 언어 모델에 적용하여 효과를 검증
- 캡셔닝(생성작업)과 retrieval(판별작업) 둘 다에서 성능이 향상됨을 보여줌


# Review

- 부족한 uniformity와 alignment가 trasferability를 해친다는 문제 제기 -> 이를 증가시키기 위한 제안 방식과 이론적 근거 -> 다양한 downstream task에서의 검증으로 잘 구성된 논문
- 멀티모달리티에서 다양한 Mixup방식을 구성하여 실험한 점은 좋았으나, 각각의 영향에 대한 ablation study가 있었으면 좋았을 듯 함.
- hard negative sample이 단순히 직관적으로 discriminative한 표현을 학습하는데 도움을 주는 것에서 넘어 이론적으로 alignment과 uniformity 증가에 어떤 이점을 제공하는지에 대한 분석을 수행
    - 학습 중 hard negative sample을 지속적으로 구성하는 것이 중요할 수 있음
- 