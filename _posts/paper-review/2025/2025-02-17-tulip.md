---
title: "[논문 리뷰] TULIP: Token-length Upgraded CLIP"
author: invhun
date: 2025-02-17 16:00:00 +0900
last_modified_at: 2025-02-17 16:00:00 +0900
categories: [Paper Review, Multimodal Learning]
tags: [Text-Image, Retrieval, Text-Image Retrieval, Multimodal Learning, Representation Learning, KD, Overcoming CLIP Limitations]
use_math: true
pin: false
---

> TULIP: Token-length Upgraded CLIP     
> ICLR 2025 Poser   
> **Ivona Najdenkoska**, Mohammad Mahdi Derakhshani, Yuki M. Asano, Nanne van Noord, Marcel Worring, Cees G. M. Snoek   
> University of Amsterdam   
> [[paper](https://arxiv.org/pdf/2410.10034)]


# 1. Abstract & Introduction


### 기존 연구 문제점
- CLIP은 최대 77개의 토큰으로 제한되어 있어, 긴 텍스트를 처리하지 못함
- 기존 방법(Long-CLIP)은 이러한 문제를 다루지만, 여전히 절대 인코딩에 의존하며, 이는 토큰 간 관계 모델링에 어려움이 있음
- 더 유연한 위치 인코딩 방법이 NLP에서 효과적이나, 비전-언어 모델에서는 탐구되지 않았으며, 계산 비용이 많이 드는 재학습 작업을 필요로함

### 제안 방법
- 긴 캡션을 위한, 상대 위치 인코딩을 가진 첫 번째 비전-언어 모델 TULIP을 제안함
- 많은 비용이 필요한 멀티모달 재학습 대신, 상대 위치 인코딩을 가진 새로운 모델로 증류하는 방식을 제안
- 긴 캡션 retrieval을 위한 새로운 벤치마크 Long-DCI를 제안함

# 2. Related Works

### Position Encodings in Transformer Models.
- NLP에서 처음 절대 위치 인코딩이 제안되었음
- 모델이 복잡해짐에 따라, "Relative positional encodings", "Randomized positional encoding", "extrapolation techniques", "positional interpolation"과 같은 방식이 등장함
- 최근 Contextual Position Encoding(CoPE)가 등장하였고, 이는 특정 단어, 명사 또는 문장에 주의를 기울일 수 있는 general한 방식임    
-> ICLR 2025에서 리젝됨
- 비전-언어 모델에서 모달리티간 위치 정보 통합은 여전히 해결되지 않았으며, 이 논문의 주된 초점임

### Contrastive Vision-Language Models with Long Captions.
- DCI는 77토큰 제한과이 모델이 상세한 캡션을 받아들이는데 제약을 준다고 지적
- DreamLIP은 긴 캡션 전체를 처리하는 대신, 긴 캡션에서 추출된 짧은 캡션을 사용함
- Long-CLIP은 절대 위치 인코딩을 보간하지만, 이는 기존 정보를 단순히 확장할 뿐, 그 기능을 근본적으로 바꾸지는 못하기 때문에, 세밀한 상대 위치를 포착하는 능력이 저하되고, 일반화가 부족함

# 3. TULIP

## 3.2. Positional Encoding Swapping

![fig1](https://1drv.ms/i/c/af5642aec05791fb/IQcKWsc_laVFSbuUppCnai4eAZeIPwh5h1T-aGOJVvxvfNQ?width=1604&height=812)

- Rotary Positonal Encoding (RoPE)는 시퀀스 내 각 위치에 고정된 벡터를 할당하는 대신, 토큰 간 상대적 거리 기반으로 임베딩을 회전 시킴

![fo1](https://1drv.ms/i/c/af5642aec05791fb/IQtb1EO65GDsQryccQrBv1wDAfPvxoYBlEly1o4ALz88XQw?width=1058&height=76)

![fo2](https://1drv.ms/i/c/af5642aec05791fb/IQcVS1bh3PxjRqAAAUuJpykBAavyR2SojOhHS2910ZRqL6Q?width=1152&height=64)
- 기존 쿼리와 키 값을 계산할 때 회전 매트릭스 $R$을 곱해줌
- $\theta$는 임베딩의 각 차원과 관련된 회전 주파수로, 토큰 임베딩의 서로 다른 차원이 다르게 회전하도록 하여, self-attention에서 절대 및 상대 위치 정보를 모두 포함함.
- 그림 1처럼, 각도 차이를 통해 상대적인 위치를 알 수 있으며, 자체적으로 절대적인 위치 정보를 포함함
- 또한 각 차원이 다르게 회전하기 때문에, 특정 차원이 360도 회전하여 원래 위치로 돌아오더라도, 다른 차원의 위치는 다르기 떄문에, 구별이 가능함

## 3.3. Relative Position Distillation

![fig2](https://1drv.ms/i/c/af5642aec05791fb/IQttdypMEl0hSJ5bsfjHYqUKAZnw1A6kVWZ6fWcJHVSTo0w?width=1622&height=858)

![fo3](https://1drv.ms/i/c/af5642aec05791fb/IQtxr6IEy7xER60MpbthiJyIAZ-lynsrdXeb2QJX7lFJoSo?width=1006&height=118)
- 상대적 위치 인코딩으로 초기화한 학생 모델로 선생 모델의 지식을 증류하는 방식을 사용
- 두 모델의 출력 임베딩 간 코사인 유사도를 측정하여, 차이를 증류 로스로 사용
- 이 단계에서는 상대적 위치 인코딩으로 77개의 토큰을 인코딩 할 수 있음

## 3.4. Relative Position Expansion

![fo4](https://1drv.ms/i/c/af5642aec05791fb/IQt3TnjgrKmGSKqAg6nY3Sb4AcapLMWoU04VQqLKYQjz5zw?width=1190&height=64)
- 학생 모델의 가중치를 그대로 복사하고, $\theta$를 다음과 같이 $(\alpha * T_g/T_f) - (\alpha -1)$ 스케일링을 수행
- 짧은 문장(기존 $\theta$)과 긴 문장(스케일링한 $\theta$)을 위한 두 개의 contrasitive loss를 사용하여, 파인튜닝을 수행함

## 4. Experiments & Results

### Datasets and Downstream tasks.
- short text-image retrieval:   
    - COCO2017 5k valid set
    - Flickr 30K All

- long text-image retrieval   
    - ShareGPT4V test set, 
    - Urban-1K
    - 각각 1천개의 이미지-캡션 쌍으로 전자는 성능이 포화상태인 in-distribution 데이터셋이며, 후자는 좁게 정의된 장면에 집중하여, 데이터셋의 다양성이 부족함
    - Dense Captioning Images (DCI) 데이터셋을 기반으로 한 long-DCI 벤치마크를 도입
    - 7000개의 이미지와 human-annotation 쌍을 포함하며, 평균길이는 200토큰

- text-image 생성

### Trainig details
- shareGPT4V 데이터셋으로 학습(Long-CLIP과 동일)
- $\alpha$: 8
- Long-CLIP과 공정한 비교를 위해 248개의 토큰을 사용하지만, 이와 달리 더 많은 토큰 길이를 사용 가능함
- 코드는 공개 예정

## 4.1. Cross-model Retrieval Comparison
![tab1](https://1drv.ms/i/c/af5642aec05791fb/IQthI94uT9-ISKj3EFtvGH--AVDTg9jGQWtPU_JEfASoZR0?width=1602&height=764)

![tab2](https://1drv.ms/i/c/af5642aec05791fb/IQTmAeOExM5bRr9V4SflXu3_AcP8VYNO4jOvz_NRLwNS9P8?width=1616&height=778)

- 긴 캡션에서는 두 백본에서 모두, 기존 방법들의 성능을 큰 차이로 초과함
- 짧은 캡션에서는 Long-CLIP의 첫 20토큰에 대해 맞춤형 접근 방식을 사용한것이 성능에 유리함
- TULIP은 맞춤화없이 서로 다른 캡션 길이에서의 일반화된 성능을 보여줌
- CLIP을 Fine-tuning했을 때, 왜 긴 캡션에서는 성능이 오르고, 짧은 캡션에서는 반대로 성능이 떨어지는지 이유는 모르겠음

## 4.2. Text-to-Image Generation

![fig3](https://1drv.ms/i/c/af5642aec05791fb/IQc3OXRQmr3lSq7AaXuJKLPZAR2Wd6MmjEtdFCdQXL20at0?width=1616&height=1770)
![instruction](https://1drv.ms/i/c/af5642aec05791fb/IQTyK4e0_pkUT6C8mnAVYFbMAZt-HJExtkDpb78K41ZTKI0?width=1618&height=336)
![t5](https://1drv.ms/i/c/af5642aec05791fb/IQaAwil73GGFT5Eb9T_qBUSrAWfPuaXIS4grA58mkusKYDc?width=1604&height=326)

- CLIP ViT-L-14의 텍스트 인코더를 TULIP으로 단순히 교체 (LongCLIP과 같은 방식)
- TULIP이 긴 캡션과 짧은 캡션 모두에서 CLIP과 LongCLIP이 놓치는 미세한 세부사항을 이해하고 모델링한다고 주장
- 정성 평가만으론 이를 평가하긴 어렵지만, appendix에 많은 양의 시각화 예시와 human evalution 결과를 첨부, rebuttal기간에 T5 기반 모델과의 시각화 비교 결과를 추가하였음

## 4.3. Ablation Study

### Different types of Relative Positional Encodings.

![t3](https://1drv.ms/i/c/af5642aec05791fb/IQaZB502Ub9cRY2pz_g9k17XAcdmjYmF3HD2iAVJSJ2LdCs?width=1620&height=432)
- 최근 도입된 (아카이브 기준 2024.05, 인용수 20회) Contextual Position Encoding (CoPE)와의 비교를 수행
- RoPE는 처음 학습된 문장 길이 이상으로 다양한 길이에 걸쳐 일반화되는 성능을 가지고 있는데 반해 CoPE는 시퀀스 길이가 증가할 떄 일반화하는데 어려움을 겪음
- 이로 인해 더 긴 데이터셋인 Long-DCI와 UrBan-1K에서 성능 차이가 두드러짐
- CoPE는 ICLR 2025에서 리젝되었는데, 이 논문의 영향이 있을지..?

### The impact of the caption length.
![fig4](https://1drv.ms/i/c/af5642aec05791fb/IQaVCQ4yrCNXS6-4xi_a0PlBAd8ILvB4G914O802o-_1C2c?width=1610&height=706)
- 이미지 인코더 고정 후, 텍스트 인코더만 fine-tuning하여 정확한 비교를 수행
- [77토큰, 154토큰]에서 성능 향상이 두드러짐
- 308토큰에서 성능이 정체 혹은 감소를 보이며, 이는 추가 토큰이 노이즈 또는 중복성을 초래할 수 있는 한계점을 나타냄. 또한 평균 캡션 길이가 174.02 토큰인 점과 관련이 있음 (이 부분은 좀더 분석이 필요할 것 같음)

### Benefit of using cosine distillation loss.

![t4](https://1drv.ms/i/c/af5642aec05791fb/IQtM8q2OpzzERqSIrQ3adQKhAYAVKmXqrniWQvcJMrgLpiw?width=1612&height=502)

- 다른 loss보다 코사인 loss가 우수한 성능을 보임
- 학생 모델은 교사 모델에 비해 서로 다른 크기의 임베딩을 생성할 수 있지만, 코사인 loss의 스케일 불변성은, 임베딩의 방향성 정보를 증류하는데 집중할 수 있도록 함

## 4.4. Additional Analysis

### Attention spread visualization
![fig5](https://1drv.ms/i/c/af5642aec05791fb/IQTRkzhfmGPPS5_bftS5VrKhAatKsI-jzohLtKawG1g9b98?width=1634&height=1086)

- CLS 토큰과 그 이전 토큰 간 attention 점수를 시각화
- TULIP은 LongCLIP에 비해 균일한 attention 분포를 보이며, 이는 다른 모델이 간과할 수 있는 캡션 후반부의 세부사항을 포착할 수 있음
- TULIP은 쉼표와 같은 구두점에 대한 attention을 증가시켜, 긴 텍스트를 구분 분석하고 분할하는 능력을 향상시킴

### Caption-image relevance distribution analysis
![fig6](https://1drv.ms/i/c/af5642aec05791fb/IQtnsxyl3Z0TQLYbjdkJ-hubAZDhKvFUgKbU1qoknrzkrVE?width=1608&height=718)
- 이미지에 대해 긴 캡션 내에서 관련 정보가 어디에 분포하는지를 조사
- Window size: 포함하는 토큰 수, # Windows: 보폭
- 유사성 점수가 다양한 서브 윈도우에 분포   
    -> 긴 입력 시퀀스를 처리할 수 있는 모델 필요성 강조
- 창 크기가 증가함에 따라 유사성 패턴이 집중화   
    -> 더 창이 더 응집력있고 관련성 높은 정보를 포착함
- 서로 다른 창에서의 유사성 변동성   
    -> 이미지 관련 정보의 비균일한 분포 확인


# Limitations
- TULIP의 성능은 ShareGPT4V 데이터셋의 캡션 품질에 의존함
- 상대 위치 인코딩으로 인해 긴 캡션의 처리가 가능하지만, 학습한 평균 토큰 길이에 의해 실제 토큰 길이가 제한됨.(이는 CLIP에서도 나타남 77까지가능하지만 20까지에서 좋은 성능 - LongCLIP)

# Review
- 같은 문제를 다룬 논문 Long-CLIP에 비해, 위치 인코딩에 대한 분석이 많고, 일반화 성능이 우수함
- 기존 RoPE를 특별한 변형없이 적용한것에 가까워서 노벨티 자체는 부족할 수 있다고 생각됨. (ICLR의 1번 리뷰어가 노벨티를 지적)
- 모든 연구는 데이터셋으로부터 시작한다...!
(ShareGPT4V, 2023.11, 438회 인용)
- 멀티모달 데이터셋 출시 후, NLP에서 연구된 방법들은 멀티모달에 적용한 연구가 바로 나옴. 연구엔 트랜드가 있고, 이 문제는 그래도 최근 주목을 받고있다고 보임. LongCLIP(2024.05, 79회 인용, ECCV2024), CoPE(2024.05, 20회 인용), TULIP(2024.10, 1회 인용)
- appendix말고 수정이 없는 것으로 보아, TULIP은 Long-CLIP을 보고, 연구를 시작했을지도...
- RoPE 자체는 21년에 제안됨(2021.04 arxiv, 2024 neurocomputing)

